{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f044970",
   "metadata": {},
   "source": [
    "Now we're changing things up \n",
    "We needs to use our current data and based off what we already listened to find the songs in the categories we listened to lease and re recommend them basef on:\n",
    "- users profile vector\n",
    "- each songs profile vector \n",
    "- the clustering of groups \n",
    "\n",
    "\"What behavioral patterns exist in personal listening data, and how can statistical modeling surface underexplored listening behaviors?”\n",
    "“Within underexplored behavior clusters, which songs best match the user’s preference profile?”\n",
    "---\n",
    "Engagment Scalars:\n",
    "\n",
    "play_count               → total plays\n",
    "skip_rate                → skips / plays\n",
    "completion_rate          → full listens / plays\n",
    "avg_listen_ratio         → avg(seconds_listened / song_length)\n",
    "replay_rate              → replays / plays\n",
    "---\n",
    "Temporal Behavior:\n",
    "\n",
    "recency_score             → exp decay of last play time\n",
    "time_of_day_distribution  → % morning / afternoon / night\n",
    "day_type_distribution     → % weekday / weekend\n",
    "---\n",
    "Behavioral Context:\n",
    "\n",
    "session_depth_avg         → avg position in session\n",
    "repeat_interval_avg       → avg time between replays\n",
    "---\n",
    "Ex:\n",
    "song_vector = [\n",
    "    play_count_norm,\n",
    "    skip_rate,\n",
    "    completion_rate,\n",
    "    avg_listen_ratio,\n",
    "    replay_rate,\n",
    "    recency_score,\n",
    "    weekday_ratio,\n",
    "    weekend_ratio,\n",
    "    morning_ratio,\n",
    "    evening_ratio,\n",
    "    session_depth_avg_norm\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd38630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports \n",
    "import pandas as pd \n",
    "import matplotlib as plot \n",
    "import matplotlib.pyplot as plt\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "801469be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ms/q3g72bzn7hb3nqw2hgm84bq40000gp/T/ipykernel_8221/1500770189.py:2: DtypeWarning: Columns (0: Bundle Version, 1: Grouping, 2: House ID, 3: Local Radio Station ID, 4: Radio Station Country, 5: Radio Station ID, 6: Radio Type, 7: Radio User ID, 8: Recommended Content Localization Key, 9: Shelf Content Identifier, 10: Source Radio Name, 11: Source Radio Type, 12: Transition Type) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(rf\"Apple Music Play Activity.csv\")\n"
     ]
    }
   ],
   "source": [
    "# get data \n",
    "df = pd.read_csv(rf\"Apple Music Play Activity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f398e862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean data \n",
    "\n",
    "# Step 1 — identify columns to drop // drop  where: null percentage is greater than or equal to 90% and where there is only no or 1 unique value\n",
    "nulls = df.isna().mean().sort_values(ascending=False)\n",
    "unique = df.nunique().sort_values()\n",
    "drop_cols = nulls[(nulls >= 0.9) | (unique == 0) | (unique == 1)].index\n",
    "\n",
    "# Step 2 — drop them\n",
    "newdf = df.drop(columns=drop_cols)\n",
    "\n",
    "# 3 - Drop all columns that have keywords in their name\n",
    "keywords_to_drop = [\"Container Origin\",\"Container Album Name\",\"Container Name\",\"Milliseconds Since Play\",\"Event Received Timestamp\",\"Event Timestamp\",\"Event Post Date\",\"Evaluation Variant\",\"Source Type\",\"ID\", \"Client\",\"Version\",\"Device\",\"Offline\",\"IP\",\"User's\",\"Provided\",\"Siri\",\"Display\",\"Use Listening\", \"Subscription\", \"Session Is\", \"Personalized\",\"Ownership\",\"Media Type\",\"Media Bundle\",\"Item Type\",\"Vocal\",\"Event Reason Hint\",\"Repeat Play\" ]\n",
    "newdf = newdf.drop(columns=[col for col in newdf.columns if any(k in col for k in keywords_to_drop)])\n",
    "# 4 - Drop rows where Song Name is null\n",
    "newdf = newdf.dropna(subset=[\"Song Name\"])\n",
    "newdf = newdf[~((newdf['Start Position In Milliseconds'] == 0) & (newdf['End Position In Milliseconds'] == 0) | (newdf['End Position In Milliseconds'] == 0) | (newdf['End Position In Milliseconds']).isna())]\n",
    "newdf = newdf[~((newdf['Media Duration In Milliseconds']==0) | (newdf['Media Duration In Milliseconds'].isna()))]\n",
    "\n",
    "#get rid of null event dates rows\n",
    "newdf=newdf[~(newdf['Event End Timestamp'].isna()|newdf['Event Start Timestamp'].isna())]\n",
    "# --- Convert timestamps to datetime objects ---\n",
    "newdf['Event Start Timestamp'] = pd.to_datetime(newdf['Event Start Timestamp'], utc=True, format=\"ISO8601\")\n",
    "newdf['Event End Timestamp'] = pd.to_datetime(newdf['Event End Timestamp'], utc=True, format=\"ISO8601\")\n",
    "\n",
    "# --- Convert durations to seconds ---\n",
    "newdf['Media Duration Sec'] = newdf['Media Duration In Milliseconds'] / 1000\n",
    "newdf['Play Duration Sec'] = newdf['Play Duration Milliseconds'] / 1000\n",
    "newdf['Start Position Sec'] = newdf['Start Position In Milliseconds'] / 1000\n",
    "newdf['End Position Sec'] = newdf['End Position In Milliseconds'] / 1000\n",
    "\n",
    "# --- Calculate percent of song played ---\n",
    "newdf['Percent Played'] = newdf['Play Duration Sec'] / newdf['Media Duration Sec']\n",
    "\n",
    "# --- Extract hour of day and day of week from start timestamp ---\n",
    "newdf['Hour of Day'] = newdf['Event Start Timestamp'].dt.hour\n",
    "newdf['Day of Week'] = newdf['Event Start Timestamp'].dt.day_name()\n",
    "\n",
    "# flag skipped vs finished (e.g., <70% = skipped)\n",
    "newdf['Skipped'] = newdf['Percent Played'] < 0.7\n",
    "\n",
    "#Dropping any columns with milliseconds since we have seconds\n",
    "newdf = newdf.drop(columns=[col for col in newdf.columns if any(k in col for k in [\"Milliseconds\"])])\n",
    "\n",
    "#drop rows where Percent Played is grater than seconds played because that means we played the song longer than the song was which is impossible\n",
    "newdf = newdf[newdf[\"Percent Played\"]<=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586ecd2",
   "metadata": {},
   "source": [
    "We cleaned the data! We need to identify the features that are important to helping classify our behavioral patters and turn them into vectors:\n",
    "(We need our personal profile vector, Each songs profile vector, and the clusters vectors)\n",
    "- average listening time \n",
    "- Day listen to music the most \n",
    "- Hour I listen to music the most \n",
    "- Average Media Duration I listen to \n",
    "- Which Feature Name (place I listen to music the longest: album, playlist, random, etc) \n",
    "- Song name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4f45f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178341, 0.5179739936413948, 0.48202600635860515, np.float64(0.9831011152237121), 0.9257882371412071]\n"
     ]
    }
   ],
   "source": [
    "#create user profile vector \n",
    "#average out all my songs and behaviors to make my profile \n",
    "# play_count               → total plays\n",
    "# skip_rate                → skips / plays\n",
    "# completion_rate          → full listens / plays\n",
    "# avg_listen_ratio         → avg(seconds_listened / song_length)\n",
    "# replay_rate              → replays / plays\n",
    "\n",
    "UserVector =[]\n",
    "play_count = len(newdf)\n",
    "skip_rate = len(newdf[newdf[\"Skipped\"]==True]) / play_count\n",
    "completion_rate = len(newdf[newdf[\"Skipped\"]==False]) / play_count\n",
    "avg_listen_ratio = (\n",
    "    newdf.loc[newdf[\"Skipped\"] == False, \"Play Duration Sec\"] /\n",
    "    newdf.loc[newdf[\"Skipped\"] == False, \"Media Duration Sec\"]\n",
    ").mean()\n",
    "#out of all my songs how many times did I replay any song \n",
    "replay_rate = sum(newdf[\"Song Name\"].value_counts()-1) / play_count\n",
    "\n",
    "UserVector.extend([play_count,skip_rate,completion_rate,avg_listen_ratio,replay_rate])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22676dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13235\n"
     ]
    }
   ],
   "source": [
    "#create song profile \n",
    "#for every song make a vector which is already just their row \n",
    "unique_songs = newdf[\"Song Name\"].unique()\n",
    "unique_songs_counts = newdf[\"Song Name\"].value_counts()\n",
    "song_vector_list =[] #array of all the completed song vectors \n",
    "for song in unique_songs:\n",
    "    SongVector =[]\n",
    "    play_count = unique_songs_counts[song]\n",
    "    skip_rate = len(newdf[(newdf[\"Song Name\"]==song) & (newdf[\"Skipped\"]==True)]) / play_count\n",
    "    completion_rate = len(newdf[(newdf[\"Song Name\"]==song) & (newdf[\"Skipped\"]==False)]) / play_count\n",
    "    avg_listen_ratio = (\n",
    "        newdf.loc[(newdf[\"Skipped\"] == False) & (newdf[\"Song Name\"]==song), \"Play Duration Sec\"] /\n",
    "        newdf.loc[(newdf[\"Skipped\"] == False) & (newdf[\"Song Name\"]==song), \"Media Duration Sec\"]\n",
    "    ).mean()\n",
    "    SongVector.extend([play_count,skip_rate,completion_rate,avg_listen_ratio,replay_rate])\n",
    "    song_vector_list.append(SongVector)\n",
    "print(len(song_vector_list))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26791749",
   "metadata": {},
   "source": [
    "Step B — Normalize vectors\n",
    "(important so play_count doesn’t dominate)\n",
    "StandardScale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484e3ac",
   "metadata": {},
   "source": [
    "We are going to use unsupervised to see what groupings I can get for my data, and then use an distance functions to score them based off of our song's vectors proximity to our user's vectors.\n",
    "\n",
    "Once again: we want to know based on our song vector profile and our songs profiles which song to re-recommend for a listen that would resonate with us the most? \n",
    "\n",
    "We will use a score to do this. \n",
    "\n",
    "How will we compute the score though? well the model could possibly compute the score and then return our scores and we just get the scores that have the higest match rate. \n",
    "\n",
    "If we do that then how will we rank each scalar in our vector and weight it? \n",
    "\n",
    "Let's think....\n",
    "Every song vector has a: play_count, skip_rate, completion_rate, and a avg_listen_ratio. When playing a song what matters most in terms of being similar to our users: play_count, skip_rate, completion_rate, avg_listen_ratio, and replay_rate?\n",
    "\n",
    "well we could take a tensor mapping of all of our vectors and see how close they are to the users and use the distance as a measurement for score, then get all the songs with one play and get only the points that have the smallest distace from the users vector point and return them. like top 5.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd776647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clusters and evaluate using different clustering algorithims to find behaviors that influence// unsupervised \n",
    "from sklearn import cluster,neighbors,naive_bayes\n",
    "import sklearn\n",
    "\n",
    "#KNN\n",
    "neighbors.KNeighborsClassifier\n",
    "neighbors.KNeighborsRegressor\n",
    "\n",
    "#Kmeans \n",
    "cluster.k_means()\n",
    "#DBSCAN\n",
    "cluster.dbscan()\n",
    "#Agglomerative Clustering\n",
    "cluster.AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9160364",
   "metadata": {},
   "source": [
    "Distance functions are what we'll use to see how far our song vectors are from our users vector. we can use any of the three distance measures below:\n",
    "1) Euclidean distance\n",
    "d = ||U - S||\n",
    "2) Cosine similarity (very common)\n",
    "score = cos(θ) = (U · S) / (||U|| ||S||)\n",
    "3) Manhattan distance\n",
    "sum(|U_i - S_i|)\n",
    "These are valid, standard, mathematically sound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939ccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create scoring \n",
    "#scoring based on distance in tensor space "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
